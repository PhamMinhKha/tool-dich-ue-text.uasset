#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
UAsset Text Extractor and Importer
Tr√≠ch xu·∫•t v√† import l·∫°i text t·ª´ file .uasset c·ªßa Unreal Engine
"""

import re
import json
import struct
from typing import Dict, List, Tuple, Optional
import argparse
import os

class UAssetTextExtractor:
    def __init__(self):
        self.text_entries = []
        self.original_data = b''
        self.original_file_size = 0
        self.size_offset_position = 0
        
    def extract_texts(self, file_path: str) -> Dict:
        """Tr√≠ch xu·∫•t text t·ª´ file .uasset"""
        try:
            print(f"üìÇ ƒêang ƒë·ªçc file: {file_path}")
            with open(file_path, 'rb') as f:
                self.original_data = f.read()
            
            file_size_mb = len(self.original_data) / (1024 * 1024)
            print(f"üìä K√≠ch th∆∞·ªõc file: {file_size_mb:.2f} MB ({len(self.original_data):,} bytes)")
            
            print("üîÑ ƒêang chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu binary sang text...")
            # Chuy·ªÉn ƒë·ªïi sang string ƒë·ªÉ t√¨m ki·∫øm pattern
            data_str = self.original_data.decode('utf-8', errors='ignore')
            print(f"üìù ƒê·ªô d√†i text sau chuy·ªÉn ƒë·ªïi: {len(data_str):,} k√Ω t·ª±")
            
            print("üöÄ B·∫Øt ƒë·∫ßu ph√¢n t√≠ch v√† tr√≠ch xu·∫•t text...")
            # T√¨m c√°c text entries
            text_data = self._parse_text_entries(data_str)
            
            # ƒê·ªçc th√¥ng tin k√≠ch th∆∞·ªõc file t·ª´ offset 0x20
            size_info = self._read_file_size_info()
            
            result = {
                'file_info': {
                    'original_file': file_path,
                    'file_size': len(self.original_data),
                    'total_entries': len(text_data),
                    'original_file_size': self.original_file_size,
                    'size_offset_position': self.size_offset_position
                },
                'text_entries': text_data
            }
            
            print(f"üéâ Tr√≠ch xu·∫•t ho√†n t·∫•t! T√¨m th·∫•y {len(text_data)} text entries")
            return result
            
        except Exception as e:
            print(f"‚ùå L·ªói khi ƒë·ªçc file: {e}")
            return {}
    
    def _parse_text_entries(self, data_str: str) -> List[Dict]:
        """Ph√¢n t√≠ch v√† tr√≠ch xu·∫•t c√°c text entries d·ª±a tr√™n c·∫•u tr√∫c file uasset."""
        entries = []
        entry_id = 0
        processed_texts = set() # ƒê·ªÉ tr√°nh tr√πng l·∫∑p
        original_binary_data = self.original_data
        data_len = len(original_binary_data)
        idx = 0

        print("üöÄ B·∫Øt ƒë·∫ßu ph√¢n t√≠ch file binary...")

        while idx < data_len - 4: # C·∫ßn √≠t nh·∫•t 4 byte cho ƒë·ªô d√†i
            try:
                # Th·ª≠ ƒë·ªçc UTF-8 string: <u32 len><string + 1>
                # ƒê·ªô d√†i ƒë∆∞·ª£c l∆∞u l√† little-endian integer
                str_len = struct.unpack('<i', original_binary_data[idx:idx+4])[0]
                
                # Ki·ªÉm tra ƒë·ªô d√†i h·ª£p l·ªá v√† c√≥ null terminator
                # ƒê·ªô d√†i c·ªßa chu·ªói text th·ª±c t·∫ø (kh√¥ng bao g·ªìm null terminator)
                actual_str_len = str_len -1

                if 0 < actual_str_len < 200: # Gi·ªõi h·∫°n ƒë·ªô d√†i h·ª£p l√Ω, tr√°nh ƒë·ªçc sai d·ªØ li·ªáu
                    if idx + 4 + str_len <= data_len:
                        # Ki·ªÉm tra null terminator cho UTF-8
                        if original_binary_data[idx + 4 + actual_str_len] == 0:
                            text_bytes = original_binary_data[idx+4 : idx+4+actual_str_len]
                            try:
                                text = text_bytes.decode('utf-8')
                                # L·ªçc c√°c chu·ªói kh√¥ng ph·∫£i text (v√≠ d·ª•: to√†n k√Ω t·ª± ƒë·∫∑c bi·ªát, ho·∫∑c qu√° ng·∫Øn)
                                if re.search(r'[a-zA-Z0-9]', text) and len(text.strip()) > 3 and text not in processed_texts:
                                    if not any(c in text for c in ['\x00', '\x01', '\x02', '\x03', '\x04', '\x05', '\x06', '\x07', '\x08', '\x0b', '\x0c', '\x0e', '\x0f']): # Lo·∫°i b·ªè c√°c k√Ω t·ª± control kh√¥ng mong mu·ªën
                                        entries.append({
                                            'id': entry_id,
                                            'key': f'utf8_entry_{entry_id}',
                                            'original_text': text.strip(),
                                            'translated_text': text.strip(),
                                            'language': self._detect_language(text),
                                            'position': idx,
                                            'length': str_len
                                        })
                                        processed_texts.add(text)
                                        entry_id += 1
                                        # print(f"Found UTF-8: {text.strip()} at {idx}")
                                        idx += 4 + str_len # Di chuy·ªÉn con tr·ªè qua ƒë·ªô d√†i + chu·ªói + null terminator
                                        continue
                            except UnicodeDecodeError:
                                pass # Kh√¥ng ph·∫£i UTF-8 h·ª£p l·ªá
                
                # Th·ª≠ ƒë·ªçc UTF-16 string: <u32 len^0xFF><(string + 1)/2>
                # ƒê·ªô d√†i ƒë∆∞·ª£c l∆∞u l√† little-endian integer, XOR v·ªõi 0xFFFFFFFF (ho·∫∑c -len n·∫øu l√† s·ªë √¢m)
                # Unreal Engine l∆∞u ƒë·ªô d√†i √¢m cho UTF-16
                str_len_utf16_encoded = struct.unpack('<i', original_binary_data[idx:idx+4])[0]
                
                if str_len_utf16_encoded < 0: # ƒê·ªô d√†i √¢m l√† d·∫•u hi·ªáu c·ªßa UTF-16
                    actual_num_chars = -str_len_utf16_encoded
                    byte_len_utf16 = actual_num_chars * 2 # M·ªói k√Ω t·ª± UTF-16 l√† 2 bytes

                    if 0 < actual_num_chars < 200: # Gi·ªõi h·∫°n ƒë·ªô d√†i h·ª£p l√Ω
                        if idx + 4 + byte_len_utf16 <= data_len:
                             # Ki·ªÉm tra null terminator cho UTF-16 (2 bytes 00 00)
                            if original_binary_data[idx + 4 + byte_len_utf16 - 2 : idx + 4 + byte_len_utf16] == b'\x00\x00':
                                text_bytes = original_binary_data[idx+4 : idx+4+byte_len_utf16-2] # B·ªè qua 2 byte null terminator
                                try:
                                    text = text_bytes.decode('utf-16-le')
                                    if re.search(r'[a-zA-Z0-9]', text) and len(text.strip()) > 3 and text not in processed_texts:
                                        if not any(c in text for c in ['\x00', '\x01', '\x02', '\x03', '\x04', '\x05', '\x06', '\x07', '\x08', '\x0b', '\x0c', '\x0e', '\x0f']): # Lo·∫°i b·ªè c√°c k√Ω t·ª± control kh√¥ng mong mu·ªën
                                            entries.append({
                                                'id': entry_id,
                                                'key': f'utf16_entry_{entry_id}',
                                                'original_text': text.strip(),
                                                'translated_text': text.strip(),
                                                'language': self._detect_language(text),
                                                'position': idx,
                                                'length': byte_len_utf16
                                            })
                                            processed_texts.add(text)
                                            entry_id += 1
                                            # print(f"Found UTF-16: {text.strip()} at {idx}")
                                            idx += 4 + byte_len_utf16 # Di chuy·ªÉn con tr·ªè
                                            continue
                                except UnicodeDecodeError:
                                    pass # Kh√¥ng ph·∫£i UTF-16 h·ª£p l·ªá

            except struct.error:
                # Kh√¥ng th·ªÉ unpack 4 bytes, c√≥ th·ªÉ ƒë√£ ƒë·∫øn cu·ªëi file ho·∫∑c d·ªØ li·ªáu kh√¥ng h·ª£p l·ªá
                pass
            except Exception as e:
                # print(f"Error parsing at {idx}: {e}")
                pass # B·ªè qua l·ªói v√† ti·∫øp t·ª•c

            idx += 1 # Di chuy·ªÉn con tr·ªè m·ªôt byte n·∫øu kh√¥ng t√¨m th·∫•y g√¨

        print(f"üéâ Ph√¢n t√≠ch binary ho√†n t·∫•t! T√¨m th·∫•y {len(entries)} text entries.")
        
        # L·ªçc v√† s·∫Øp x·∫øp l·∫°i n·∫øu c·∫ßn, hi·ªán t·∫°i ƒë√£ l·ªçc trong v√≤ng l·∫∑p
        # S·∫Øp x·∫øp theo v·ªã tr√≠ ƒë·ªÉ ƒë·∫£m b·∫£o th·ª© t·ª±
        entries.sort(key=lambda e: e['position'])
        # C·∫≠p nh·∫≠t l·∫°i ID sau khi s·∫Øp x·∫øp
        for i, entry in enumerate(entries):
            entry['id'] = i
            entry['key'] = f"{entry['key'].split('_')[0]}_entry_{i}" # C·∫≠p nh·∫≠t key v·ªõi id m·ªõi

        return entries
    
    def _detect_language(self, text: str) -> str:
        """Ph√°t hi·ªán ng√¥n ng·ªØ c·ªßa text"""
        text_lower = text.lower()
        words = text_lower.split()
        
        # Ki·ªÉm tra c√°c k√Ω t·ª± ƒë·∫∑c tr∆∞ng theo th·ª© t·ª± ∆∞u ti√™n
     
        
        # 2. Ti·∫øng Nh·∫≠t
        if re.search(r'[„Å≤„Çâ„Åå„Å™„Ç´„Çø„Ç´„Éä]', text):
            return 'japanese'
        
        # 3. Ti·∫øng H√†n
        elif re.search(r'[Í∞Ä-Ìû£]', text):
            return 'korean'
        
        # 4. Ti·∫øng Trung
        elif re.search(r'[‰∏Ä-ÈæØ]', text):
            return 'chinese'
        
        # 5. Ti·∫øng ƒê·ª©c (ki·ªÉm tra k√Ω t·ª± ƒë·∫∑c bi·ªát)
        elif re.search(r'[√§√∂√º√ü]', text_lower):
            return 'german'
        
        # 6. Ki·ªÉm tra t·ª´ kh√≥a ƒë·∫∑c tr∆∞ng ri√™ng bi·ªát
        # ƒê·∫øm s·ªë t·ª´ kh√≥a c·ªßa m·ªói ng√¥n ng·ªØ
        german_words = ['der', 'die', 'das', 'und', 'ist', 'nicht', 'ein', 'eine', 'zu', 'auf', 'mit', 'sich', 'auch', 'noch', 'alle', 'sein', 'werden', 'haben', 'k√∂nnen', 'm√ºssen', 'sollen', 'wollen', 'scheinen', 'jedoch']
        french_words = ['le', 'la', 'les', 'du', 'des', 'et', 'est', 'une', 'dans', 'pour', 'que', 'qui', 'avec', 'sur', 'par', 'tout', 'tous', 'cette', 'ces', 'mais', 'pas', 'tr√®s', '√™tre', 'avoir', 'faire', 'aller', 'voir', 'savoir', 'pouvoir', 'vouloir', 'devoir']
        italian_words = ['il', 'lo', 'gli', 'della', 'dello', 'degli', 'delle', 'con', 'per', 'tra', 'fra', 'che', 'non', 'una', 'uno', 'questo', 'questa', 'quello', 'quella', 'essere', 'avere', 'fare', 'dire', 'andare', 'potere', 'dovere', 'volere', 'sapere', 'dare', 'ciao', 'mondo', 'come', 'dove', 'quando', 'perch√©', 'molto', 'bene', 'male', 'grande', 'piccolo']
        spanish_words = ['el', 'los', 'las', 'del', 'por', 'para', 'no', 'este', 'esta', 'ese', 'esa', 'aquel', 'aquella', 'ser', 'estar', 'tener', 'hacer', 'decir', 'ir', 'poder', 'deber', 'querer', 'saber', 'dar', 'hola', 'mundo', 'como', 'donde', 'cuando', 'porque', 'muy', 'bien', 'mal', 'grande', 'peque√±o']
        english_words = ['the', 'and', 'you', 'that', 'what', 'with', 'have', 'this', 'will', 'your', 'from', 'they', 'know', 'want', 'been', 'good', 'much', 'some', 'time', 'very', 'when', 'come', 'here', 'just', 'like', 'long', 'make', 'many', 'over', 'such', 'take', 'than', 'them', 'well', 'were', 'buddy', 'found', 'heard', 'water', 'hit', 'lets']
        
        # ƒê·∫øm t·ª´ kh√≥a cho m·ªói ng√¥n ng·ªØ
        german_count = sum(1 for word in words if word in german_words)
        french_count = sum(1 for word in words if word in french_words)
        italian_count = sum(1 for word in words if word in italian_words)
        spanish_count = sum(1 for word in words if word in spanish_words)
        english_count = sum(1 for word in words if word in english_words)
        
        # T√¨m ng√¥n ng·ªØ c√≥ nhi·ªÅu t·ª´ kh√≥a nh·∫•t
        language_scores = {
            'german': german_count,
            'french': french_count,
            'italian': italian_count,
        'spanish': spanish_count,
            'english': english_count
        }
        
        # N·∫øu c√≥ t·ª´ kh√≥a ƒë∆∞·ª£c t√¨m th·∫•y, tr·∫£ v·ªÅ ng√¥n ng·ªØ c√≥ ƒëi·ªÉm cao nh·∫•t
        max_score = max(language_scores.values())
        if max_score > 0:
            for lang, score in language_scores.items():
                if score == max_score:
                    return lang
        
        # 7. Ki·ªÉm tra k√Ω t·ª± c√≥ d·∫•u cho c√°c ng√¥n ng·ªØ ch√¢u √Çu (fallback)
        elif re.search(r'[√†√¢√§√©√®√™√´√Ø√Æ√¥√∂√π√ª√º√ø√ß]', text_lower):
            return 'french'
        elif re.search(r'[√†√®√¨√≤√π√°√©√≠√≥√∫]', text_lower):
            return 'italian'
        elif re.search(r'[√±√°√©√≠√≥√∫√º]', text_lower):
            return 'spanish'
        
        # 8. M·∫∑c ƒë·ªãnh l√† ti·∫øng Anh
        else:
            return 'english'
    
    def export_to_json(self, extracted_data: Dict, output_file: str):
        """Xu·∫•t d·ªØ li·ªáu ra file JSON ƒë·ªÉ ch·ªânh s·ª≠a"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(extracted_data, f, ensure_ascii=False, indent=2)
            print(f"ƒê√£ xu·∫•t d·ªØ li·ªáu ra: {output_file}")
        except Exception as e:
            print(f"L·ªói khi xu·∫•t file JSON: {e}")
    
    def import_from_json(self, json_file: str) -> Dict:
        """ƒê·ªçc d·ªØ li·ªáu ƒë√£ ch·ªânh s·ª≠a t·ª´ file JSON"""
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"L·ªói khi ƒë·ªçc file JSON: {e}")
            return {}
    
    def _read_file_size_info(self):
        """ƒê·ªçc th√¥ng tin k√≠ch th∆∞·ªõc file t·ª´ offset 0x20"""
        try:
            if len(self.original_data) >= 0x24:  # C·∫ßn √≠t nh·∫•t 0x24 bytes
                # ƒê·ªçc 4 bytes t·∫°i offset 0x20 ƒë·ªÉ l·∫•y offset c·ªßa v·ªã tr√≠ l∆∞u k√≠ch th∆∞·ªõc
                self.size_offset_position = struct.unpack('<I', self.original_data[0x20:0x24])[0]
                print(f"üìç Size offset position: {self.size_offset_position} (0x{self.size_offset_position:X})")
                
                if self.size_offset_position > 0 and self.size_offset_position < len(self.original_data):
                    # T√≠nh k√≠ch th∆∞·ªõc t·ª´ size_offset_position ƒë·∫øn cu·ªëi file, tr·ª´ th√™m 104
                    self.original_file_size = len(self.original_data) - self.size_offset_position - 104
                    print(f"üìè Original file size: {self.original_file_size} bytes (t√≠nh t·ª´ offset 0x{self.size_offset_position:X} ƒë·∫øn cu·ªëi file, tr·ª´ 104)")
                    print(f"üìè C√¥ng th·ª©c: {len(self.original_data)} - {self.size_offset_position} - 104 = {self.original_file_size}")
                    return True
                else:
                    print(f"‚ö†Ô∏è Size offset position kh√¥ng h·ª£p l·ªá: {self.size_offset_position} (file size: {len(self.original_data)})")
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ ƒë·ªçc th√¥ng tin k√≠ch th∆∞·ªõc file: {e}")
            self.original_file_size = len(self.original_data)
            self.size_offset_position = 0
        return False
    
    def _insert_data_at_position(self, data: bytearray, position: int, new_data: bytes, old_length: int) -> tuple[bytearray, int]:
        """Thay th·∫ø d·ªØ li·ªáu t·∫°i v·ªã tr√≠ c·ª• th·ªÉ v·ªõi dynamic resizing
        Returns: (updated_data, size_change)
        """
        try:
            new_length = len(new_data)
            size_change = new_length - old_length
            
            print(f"üîÑ Thay th·∫ø t·∫°i 0x{position:X}: {old_length} -> {new_length} bytes (thay ƒë·ªïi: {size_change:+d})")
            
            # B∆∞·ªõc 1: X√≥a to√†n b·ªô v√πng c≈©
            del data[position:position + old_length]
            print(f"üóëÔ∏è ƒê√£ x√≥a {old_length} bytes t·∫°i v·ªã tr√≠ 0x{position:X}")
            
            # B∆∞·ªõc 2: Ch√®n d·ªØ li·ªáu m·ªõi v√†o v·ªã tr√≠ ƒë√≥
            data[position:position] = new_data
            print(f"‚úèÔ∏è ƒê√£ ch√®n {new_length} bytes t·∫°i v·ªã tr√≠ 0x{position:X}")
            
            return data, size_change
            
        except Exception as e:
            print(f"‚ùå L·ªói khi thay th·∫ø d·ªØ li·ªáu: {e}")
            import traceback
            traceback.print_exc()
            return data, 0
    
    def _calculate_new_file_size(self, new_data: bytearray, size_offset_position: int = None) -> int:
        """T√≠nh to√°n k√≠ch th∆∞·ªõc file m·ªõi t·ª´ size_offset_position ƒë·∫øn cu·ªëi file, tr·ª´ th√™m 104"""
        try:
            if size_offset_position is not None and size_offset_position > 0:
                # T√≠nh t·ª´ size_offset_position ƒë·∫øn cu·ªëi file, tr·ª´ th√™m 104
                new_size = len(new_data) - size_offset_position - 104
                print(f"üìê T√≠nh to√°n k√≠ch th∆∞·ªõc m·ªõi t·ª´ offset 0x{size_offset_position:X}: {len(new_data)} - {size_offset_position} - 104 = {new_size}")
                return new_size
            else:
                # Fallback: t√≠nh theo c√°ch c≈©
                # Ki·ªÉm tra 4 byte cu·ªëi c√≥ ph·∫£i l√† C1 83 2A 9E kh√¥ng
                if len(new_data) >= 4:
                    last_4_bytes = new_data[-4:]
                    expected_bytes = bytes([0xC1, 0x83, 0x2A, 0x9E])
                    
                    if last_4_bytes == expected_bytes:
                        new_size = len(new_data) - 4 - 100
                        print(f"üìê T√≠nh to√°n k√≠ch th∆∞·ªõc m·ªõi (fallback): {len(new_data)} - 4 - 100 = {new_size}")
                        return new_size
                    else:
                        print(f"‚ö†Ô∏è 4 byte cu·ªëi kh√¥ng ph·∫£i C1 83 2A 9E: {last_4_bytes.hex().upper()}")
                
                # Fallback cu·ªëi c√πng: ch·ªâ tr·ª´ 100
                new_size = len(new_data) - 100
                print(f"üìê T√≠nh to√°n k√≠ch th∆∞·ªõc m·ªõi (fallback): {len(new_data)} - 100 = {new_size}")
                return new_size
            
        except Exception as e:
            print(f"‚ùå L·ªói khi t√≠nh to√°n k√≠ch th∆∞·ªõc m·ªõi: {e}")
            return len(new_data)

    def rebuild_uasset(self, json_data: Dict, output_file: str):
        """T√°i t·∫°o file .uasset v·ªõi text ƒë√£ ch·ªânh s·ª≠a, c·∫≠p nh·∫≠t ƒë√∫ng len v√† size t·ªïng cho UTF-8/UTF-16 v·ªõi dynamic resizing"""
        try:
            # B·∫Øt ƒë·∫ßu v·ªõi d·ªØ li·ªáu g·ªëc
            new_data = bytearray(self.original_data)
            
            # L·∫•y th√¥ng tin k√≠ch th∆∞·ªõc t·ª´ JSON n·∫øu c√≥
            file_info = json_data.get('file_info', {})
            original_file_size = file_info.get('original_file_size', self.original_file_size)
            size_offset_position = file_info.get('size_offset_position', self.size_offset_position)
            
            # Theo d√µi t·ªïng thay ƒë·ªïi k√≠ch th∆∞·ªõc ƒë·ªÉ c·∫≠p nh·∫≠t c√°c offset sau
            total_size_change = 0
            processed_entries = []
            
            # S·∫Øp x·∫øp entries theo position ƒë·ªÉ x·ª≠ l√Ω t·ª´ cu·ªëi l√™n ƒë·∫ßu (tr√°nh ·∫£nh h∆∞·ªüng offset)
            sorted_entries = sorted(json_data.get('text_entries', []), key=lambda x: x['position'], reverse=True)
            
            for entry in sorted_entries:
                original_text = entry['original_text']
                translated_text = entry['translated_text']
                position = entry['position']
                length = entry['length']
                key = entry['key']
                
                if original_text == translated_text:
                    continue
                    
                print(f"\nüîÑ X·ª≠ l√Ω entry t·∫°i 0x{position:X}: '{original_text}' -> '{translated_text}'")
                
                if key.startswith('utf8_entry'):
                    # <u32 len><string + 1>
                    new_bytes = translated_text.encode('utf-8') + b'\x00'
                    new_len = len(new_bytes)
                    # length ƒë√£ bao g·ªìm c·∫£ null terminator, v·∫≠y old_text_length = length
                    old_text_length = length
                    
                    # C·∫≠p nh·∫≠t length field
                    new_data[position:position+4] = struct.pack('<i', new_len)
                    
                    # S·ª≠ d·ª•ng dynamic resizing cho text data
                    new_data, size_change = self._insert_data_at_position(
                        new_data, position + 4, new_bytes, old_text_length
                    )
                    total_size_change += size_change
                    
                elif key.startswith('utf16_entry'):
                    # <u32 len^0xFF><(string + 1)/2>
                    new_bytes = translated_text.encode('utf-16-le') + b'\x00\x00'
                    new_len = len(new_bytes) // 2
                    # length ƒë√£ bao g·ªìm c·∫£ null terminator, v·∫≠y old_text_length = length
                    old_text_length = length
                    
                    # C·∫≠p nh·∫≠t length field (√¢m, little-endian)
                    new_data[position:position+4] = struct.pack('<i', -new_len)
                    
                    # S·ª≠ d·ª•ng dynamic resizing cho text data
                    new_data, size_change = self._insert_data_at_position(
                        new_data, position + 4, new_bytes, old_text_length
                    )
                    total_size_change += size_change
                
                processed_entries.append({
                    'position': position,
                    'old_text': original_text,
                    'new_text': translated_text,
                    'size_change': size_change if 'size_change' in locals() else 0
                })
            
            print(f"\nüìä T·ªïng k·∫øt thay ƒë·ªïi k√≠ch th∆∞·ªõc: {total_size_change} bytes")
            print(f"üìè K√≠ch th∆∞·ªõc file: {len(self.original_data)} -> {len(new_data)} bytes")
            
            # C·∫≠p nh·∫≠t k√≠ch th∆∞·ªõc file m·ªõi v·ªõi dynamic sizing
            if size_offset_position > 0:
                # T√≠nh v·ªã tr√≠ th·ª±c t·∫ø ƒë·ªÉ ghi k√≠ch th∆∞·ªõc (offset + 8)
                actual_size_position = size_offset_position + 8
                if actual_size_position + 4 <= len(new_data):
                    # T√≠nh v·ªã tr√≠ b·∫Øt ƒë·∫ßu t√≠nh to√°n k√≠ch th∆∞·ªõc (actual_size_position + 4)
                    start_calc_position = actual_size_position + 4
                    new_file_size = self._calculate_new_file_size(new_data, start_calc_position)
                    new_data[actual_size_position:actual_size_position+4] = struct.pack('<I', new_file_size)
                    print(f"üìù ƒê√£ c·∫≠p nh·∫≠t k√≠ch th∆∞·ªõc file t·∫°i offset 0x{actual_size_position:X} (0x{size_offset_position:X} + 8): {original_file_size} -> {new_file_size}")
                    print(f"üìè T√≠nh k√≠ch th∆∞·ªõc t·ª´ v·ªã tr√≠: 0x{start_calc_position:X}")
                    print(f"üîÑ Thay ƒë·ªïi t·ªïng c·ªông: {total_size_change} bytes")
                else:
                    print(f"‚ö†Ô∏è V·ªã tr√≠ ghi k√≠ch th∆∞·ªõc kh√¥ng h·ª£p l·ªá: 0x{actual_size_position:X} v∆∞·ª£t qu√° k√≠ch th∆∞·ªõc file")
            
            # Ghi file output v·ªõi k√≠ch th∆∞·ªõc m·ªõi
            with open(output_file, 'wb') as f:
                f.write(new_data)
            
            print(f"\n‚úÖ ƒê√£ t·∫°o file m·ªõi: {output_file}")
            print(f"üìà K√≠ch th∆∞·ªõc thay ƒë·ªïi: {len(self.original_data)} -> {len(new_data)} bytes ({total_size_change:+d})")
            
            # Hi·ªÉn th·ªã th·ªëng k√™ chi ti·∫øt
            if processed_entries:
                print(f"\nüìã Chi ti·∫øt {len(processed_entries)} entries ƒë√£ x·ª≠ l√Ω:")
                for i, entry in enumerate(processed_entries[:5], 1):  # Ch·ªâ hi·ªÉn th·ªã 5 entries ƒë·∫ßu
                    print(f"  {i}. 0x{entry['position']:X}: '{entry['old_text'][:20]}...' -> '{entry['new_text'][:20]}...' ({entry['size_change']:+d} bytes)")
                if len(processed_entries) > 5:
                    print(f"  ... v√† {len(processed_entries) - 5} entries kh√°c")
                    
        except Exception as e:
            print(f"‚ùå L·ªói khi t√°i t·∫°o file .uasset: {e}")
            import traceback
            traceback.print_exc()

def batch_extract_all():
    """Tr√≠ch xu·∫•t t·∫•t c·∫£ file .uasset trong folder hi·ªán t·∫°i v√† folder 'original' ra folder 'extract'"""
    import time
    
    extractor = UAssetTextExtractor()
    
    # T·∫°o folder extract n·∫øu ch∆∞a c√≥
    extract_folder = "extract"
    if not os.path.exists(extract_folder):
        os.makedirs(extract_folder)
        print(f"üìÅ ƒê√£ t·∫°o folder: {extract_folder}")
    
    # T√¨m t·∫•t c·∫£ file .uasset trong folder hi·ªán t·∫°i
    uasset_files = [f for f in os.listdir('.') if f.endswith('.uasset')]
    
    # T√¨m th√™m file .uasset trong folder "original" n·∫øu c√≥
    original_folder = "original"
    if os.path.exists(original_folder):
        original_files = [os.path.join(original_folder, f) for f in os.listdir(original_folder) if f.endswith('.uasset')]
        uasset_files.extend(original_files)
        print(f"üìÅ T√¨m th·∫•y th√™m {len(original_files)} file trong folder original")
    
    if not uasset_files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file .uasset n√†o trong folder hi·ªán t·∫°i v√† folder original")
        return
    
    print(f"\nüìã Danh s√°ch {len(uasset_files)} file .uasset s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω:")
    for i, file in enumerate(uasset_files, 1):
        file_size = os.path.getsize(file) / (1024 * 1024) if os.path.exists(file) else 0
        print(f"  {i:2d}. {file} ({file_size:.2f} MB)")
    
    print(f"\nüöÄ B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t {len(uasset_files)} file...")
    print("=" * 60)
    
    start_time = time.time()
    success_count = 0
    total_entries = 0
    
    for i, uasset_file in enumerate(uasset_files, 1):
        try:
            file_start_time = time.time()
            print(f"\nüìÅ [{i}/{len(uasset_files)}] ƒêang x·ª≠ l√Ω: {uasset_file}")
            
            # Tr√≠ch xu·∫•t text
            extracted_data = extractor.extract_texts(uasset_file)
            
            if extracted_data and extracted_data.get('text_entries'):
                # T·∫°o t√™n file JSON trong folder extract
                json_filename = os.path.basename(uasset_file).replace('.uasset', '_texts.json')
                json_path = os.path.join(extract_folder, json_filename)
                
                # Xu·∫•t ra file JSON
                extractor.export_to_json(extracted_data, json_path)
                
                entry_count = len(extracted_data.get('text_entries', []))
                total_entries += entry_count
                file_time = time.time() - file_start_time
                
                print(f"  ‚úÖ Th√†nh c√¥ng: {entry_count} text entries -> {json_path}")
                print(f"  ‚è±Ô∏è  Th·ªùi gian x·ª≠ l√Ω: {file_time:.2f}s")
                success_count += 1
            else:
                print(f"  ‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y text c√≥ √Ω nghƒ©a trong {uasset_file}")
            
            # T√≠nh to√°n th·ªùi gian ∆∞·ªõc t√≠nh c√≤n l·∫°i
            elapsed_time = time.time() - start_time
            if i > 0:
                avg_time_per_file = elapsed_time / i
                remaining_files = len(uasset_files) - i
                estimated_remaining = avg_time_per_file * remaining_files
                print(f"  üìä Ti·∫øn tr√¨nh: {i}/{len(uasset_files)} ({i/len(uasset_files)*100:.1f}%)")
                print(f"  ‚è∞ Th·ªùi gian ∆∞·ªõc t√≠nh c√≤n l·∫°i: {estimated_remaining/60:.1f} ph√∫t")
                
        except Exception as e:
            print(f"  ‚ùå L·ªói khi x·ª≠ l√Ω {uasset_file}: {e}")
            import traceback
            print(f"  üîç Chi ti·∫øt l·ªói: {traceback.format_exc()}")
    
    total_time = time.time() - start_time
    print("\n" + "=" * 60)
    print(f"üéâ HO√ÄN TH√ÄNH! K·∫øt qu·∫£ t·ªïng k·∫øt:")
    print(f"  ‚úÖ Th√†nh c√¥ng: {success_count}/{len(uasset_files)} file")
    print(f"  üìù T·ªïng text entries: {total_entries:,}")
    print(f"  ‚è±Ô∏è  T·ªïng th·ªùi gian: {total_time/60:.2f} ph√∫t")
    print(f"  üìÇ C√°c file JSON ƒë√£ ƒë∆∞·ª£c l∆∞u trong folder: {extract_folder}")
    
    if success_count < len(uasset_files):
        failed_count = len(uasset_files) - success_count
        print(f"  ‚ö†Ô∏è  {failed_count} file kh√¥ng th·ªÉ x·ª≠ l√Ω - ki·ªÉm tra log ·ªü tr√™n ƒë·ªÉ bi·∫øt chi ti·∫øt")

def batch_import_all():
    """Import t·∫•t c·∫£ file JSON t·ª´ folder 'extract' v√† t·∫°o file .uasset m·ªõi trong folder 'import'"""
    extractor = UAssetTextExtractor()
    
    extract_folder = "extract"
    import_folder = "import"
    
    # Ki·ªÉm tra folder extract
    if not os.path.exists(extract_folder):
        print(f"Kh√¥ng t√¨m th·∫•y folder: {extract_folder}")
        print("H√£y ch·∫°y 'batch-extract' tr∆∞·ªõc")
        return
    
    # T·∫°o folder import n·∫øu ch∆∞a c√≥
    if not os.path.exists(import_folder):
        os.makedirs(import_folder)
        print(f"ƒê√£ t·∫°o folder: {import_folder}")
    
    # T√¨m t·∫•t c·∫£ file JSON trong folder extract
    json_files = [f for f in os.listdir(extract_folder) if f.endswith('_texts.json')]
    
    if not json_files:
        print(f"Kh√¥ng t√¨m th·∫•y file JSON n√†o trong folder: {extract_folder}")
        return
    
    print(f"T√¨m th·∫•y {len(json_files)} file JSON:")
    for file in json_files:
        print(f"  - {file}")
    
    print("\nB·∫Øt ƒë·∫ßu import...")
    
    success_count = 0
    for json_file in json_files:
        try:
            json_path = os.path.join(extract_folder, json_file)
            print(f"\nüìÅ ƒêang x·ª≠ l√Ω: {json_file}")
            
            # ƒê·ªçc file JSON
            json_data = extractor.import_from_json(json_path)
            if not json_data:
                print(f"  ‚ùå Kh√¥ng th·ªÉ ƒë·ªçc file JSON: {json_file}")
                continue
            
            # T√¨m file .uasset g·ªëc
            original_uasset = json_data.get('file_info', {}).get('original_file')
            if not original_uasset or not os.path.exists(original_uasset):
                print(f"  ‚ùå Kh√¥ng t√¨m th·∫•y file .uasset g·ªëc cho {json_file}")
                continue
            
            # ƒê·ªçc l·∫°i file g·ªëc
            extractor.extract_texts(original_uasset)
            
            # T·∫°o t√™n file .uasset m·ªõi trong folder import
            original_filename = os.path.basename(original_uasset)
            new_filename = original_filename.replace('.uasset', '.uasset')
            new_path = os.path.join(import_folder, new_filename)
            
            # T·∫°o file .uasset m·ªõi
            extractor.rebuild_uasset(json_data, new_path)
            
            print(f"  ‚úÖ Th√†nh c√¥ng: {new_path}")
            success_count += 1
            
        except Exception as e:
            print(f"  ‚ùå L·ªói khi x·ª≠ l√Ω {json_file}: {e}")
    
    print(f"\nüéâ Ho√†n th√†nh! ƒê√£ import th√†nh c√¥ng {success_count}/{len(json_files)} file")
    print(f"üìÇ C√°c file .uasset m·ªõi ƒë√£ ƒë∆∞·ª£c l∆∞u trong folder: {import_folder}")

def main():
    parser = argparse.ArgumentParser(description='UAsset Text Extractor and Importer')
    parser.add_argument('action', choices=['extract', 'import', 'batch-extract', 'batch-import'], 
                       help='H√†nh ƒë·ªông: extract, import, batch-extract, batch-import')
    parser.add_argument('input_file', nargs='?', help='File ƒë·∫ßu v√†o (.uasset ho·∫∑c .json) - kh√¥ng c·∫ßn cho batch operations')
    parser.add_argument('-o', '--output', help='File ƒë·∫ßu ra')
    
    args = parser.parse_args()
    
    extractor = UAssetTextExtractor()
    
    if args.action == 'batch-extract':
        # Tr√≠ch xu·∫•t t·∫•t c·∫£ file .uasset trong folder hi·ªán t·∫°i
        batch_extract_all()
        
    elif args.action == 'batch-import':
        # Import t·∫•t c·∫£ file t·ª´ folder extract
        batch_import_all()
        
    elif args.action == 'extract':
        # Tr√≠ch xu·∫•t text t·ª´ .uasset
        if not args.input_file:
            print("C·∫ßn ch·ªâ ƒë·ªãnh file ƒë·∫ßu v√†o cho action 'extract'")
            return
            
        if not args.input_file.endswith('.uasset'):
            print("File ƒë·∫ßu v√†o ph·∫£i l√† .uasset")
            return
        
        output_file = args.output or args.input_file.replace('.uasset', '_texts.json')
        
        print(f"ƒêang tr√≠ch xu·∫•t text t·ª´: {args.input_file}")
        extracted_data = extractor.extract_texts(args.input_file)
        
        if extracted_data:
            extractor.export_to_json(extracted_data, output_file)
            print(f"T√¨m th·∫•y {len(extracted_data.get('text_entries', []))} text entries")
        else:
            print("Kh√¥ng th·ªÉ tr√≠ch xu·∫•t d·ªØ li·ªáu")
    
    elif args.action == 'import':
        # Import text ƒë√£ ch·ªânh s·ª≠a v√† t·∫°o .uasset m·ªõi
        if not args.input_file:
            print("C·∫ßn ch·ªâ ƒë·ªãnh file ƒë·∫ßu v√†o cho action 'import'")
            return
            
        if not args.input_file.endswith('.json'):
            print("File ƒë·∫ßu v√†o ph·∫£i l√† .json")
            return
        
        json_data = extractor.import_from_json(args.input_file)
        if not json_data:
            print("Kh√¥ng th·ªÉ ƒë·ªçc file JSON")
            return
        
        original_uasset = json_data.get('file_info', {}).get('original_file')
        if not original_uasset or not os.path.exists(original_uasset):
            print("Kh√¥ng t√¨m th·∫•y file .uasset g·ªëc")
            return
        
        # ƒê·ªçc l·∫°i file g·ªëc
        extractor.extract_texts(original_uasset)
        
        output_file = args.output or original_uasset.replace('.uasset', '_translated.uasset')
        
        print(f"ƒêang t·∫°o file .uasset m·ªõi t·ª´: {args.input_file}")
        extractor.rebuild_uasset(json_data, output_file)

if __name__ == '__main__':
    # V√≠ d·ª• s·ª≠ d·ª•ng n·∫øu ch·∫°y tr·ª±c ti·∫øp
    if len(os.sys.argv) == 1:
        print("V√≠ d·ª• s·ª≠ d·ª•ng:")
        print("1. Tr√≠ch xu·∫•t text: python uasset_text_extractor.py extract GDSSystemText.uasset")
        print("2. Import text: python uasset_text_extractor.py import GDSSystemText_texts.json")
        print("")
        
        # Demo v·ªõi file hi·ªán t·∫°i
        extractor = UAssetTextExtractor()
        demo_file = "/Users/phamminhkha/Desktop/DichGame/GDSSystemText.uasset"
        
        if os.path.exists(demo_file):
            print(f"Demo: Tr√≠ch xu·∫•t text t·ª´ {demo_file}")
            extracted_data = extractor.extract_texts(demo_file)
            
            if extracted_data:
                output_json = demo_file.replace('.uasset', '_texts.json')
                extractor.export_to_json(extracted_data, output_json)
                print(f"ƒê√£ t·∫°o file: {output_json}")
                print(f"T√¨m th·∫•y {len(extracted_data.get('text_entries', []))} text entries")
                
                # Hi·ªÉn th·ªã m·ªôt v√†i v√≠ d·ª•
                print("\nM·ªôt v√†i text entries ƒë·∫ßu ti√™n:")
                for i, entry in enumerate(extracted_data.get('text_entries', [])[:5]):
                    print(f"{i+1}. [{entry['language']}] {entry['original_text'][:50]}...")
    else:
        main()