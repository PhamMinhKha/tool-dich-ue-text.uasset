#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
UAsset Text Extractor and Importer
Tr√≠ch xu·∫•t v√† import l·∫°i text t·ª´ file .uasset c·ªßa Unreal Engine
"""

import re
import json
import struct
from typing import Dict, List, Tuple, Optional
import argparse
import os

class UAssetTextExtractor:
    def __init__(self):
        self.text_entries = []
        self.original_data = b''
        
    def extract_texts(self, file_path: str) -> Dict:
        """Tr√≠ch xu·∫•t text t·ª´ file .uasset"""
        try:
            with open(file_path, 'rb') as f:
                self.original_data = f.read()
            
            # Chuy·ªÉn ƒë·ªïi sang string ƒë·ªÉ t√¨m ki·∫øm pattern
            data_str = self.original_data.decode('utf-8', errors='ignore')
            
            # T√¨m c√°c text entries
            text_data = self._parse_text_entries(data_str)
            
            return {
                'file_info': {
                    'original_file': file_path,
                    'file_size': len(self.original_data),
                    'total_entries': len(text_data)
                },
                'text_entries': text_data
            }
            
        except Exception as e:
            print(f"L·ªói khi ƒë·ªçc file: {e}")
            return {}
    
    def _parse_text_entries(self, data_str: str) -> List[Dict]:
        """Ph√¢n t√≠ch v√† tr√≠ch xu·∫•t c√°c text entries"""
        entries = []
        
        # T√¨m c√°c text c√≥ √Ω nghƒ©a (c√¢u ho√†n ch·ªânh, t·ª´ c√≥ nghƒ©a)
        # Pattern cho text c√≥ √Ω nghƒ©a: b·∫Øt ƒë·∫ßu b·∫±ng ch·ªØ hoa, c√≥ kho·∫£ng tr·∫Øng, d·∫•u c√¢u
        meaningful_patterns = [
            r'[A-Z][a-z]+(?:\s+[a-zA-Z]+)*[.!?]',  # C√¢u ho√†n ch·ªânh
            r'[A-Z][a-z]+(?:\s+[a-zA-Z]+){2,}',     # C·ª•m t·ª´ d√†i
            r'[A-Z][a-z]+\s+[a-z]+\s*[?]',         # C√¢u h·ªèi
            r'[A-Z][a-z]+(?:\s+[a-z]+)*\s*[:]',    # Text c√≥ d·∫•u hai ch·∫•m
        ]
        
        # T√¨m t·∫•t c·∫£ text c√≥ √Ω nghƒ©a
        all_meaningful_texts = set()
        for pattern in meaningful_patterns:
            matches = re.findall(pattern, data_str)
            all_meaningful_texts.update(matches)
        
        # T√¨m th√™m c√°c text d√†i c√≥ ch·ª©a t·ª´ th√¥ng d·ª•ng
        common_words = ['the', 'and', 'you', 'can', 'not', 'will', 'have', 'this', 'that', 'with', 'from', 'they', 'your', 'here', 'mode', 'name', 'enter', 'please']
        long_text_pattern = r'[A-Za-z][A-Za-z0-9\s\.,\?\!\-\(\)\[\]\{\}\'\"]{10,}'
        long_texts = re.findall(long_text_pattern, data_str)
        
        for text in long_texts:
            text = text.strip()
            # Ki·ªÉm tra xem c√≥ ch·ª©a t·ª´ th√¥ng d·ª•ng kh√¥ng
            text_lower = text.lower()
            if any(word in text_lower for word in common_words) and len(text) > 8:
                all_meaningful_texts.add(text)
        
        # T√¨m c√°c key/identifier
        key_pattern = r'\b([a-z]+_[a-z]+(?:_[a-z0-9]+)*)\b'
        keys = re.findall(key_pattern, data_str)
        
        # T·∫°o mapping key -> text
        current_key = None
        entry_id = 0
        processed_texts = set()  # Tr√°nh tr√πng l·∫∑p
        
        # S·∫Øp x·∫øp theo v·ªã tr√≠ xu·∫•t hi·ªán trong file
        text_positions = []
        for text in all_meaningful_texts:
            pos = data_str.find(text)
            if pos != -1 and text not in processed_texts:
                text_positions.append((pos, text))
        
        text_positions.sort()  # S·∫Øp x·∫øp theo v·ªã tr√≠
        
        for pos, text in text_positions:
            if len(text.strip()) < 5:  # B·ªè qua text qu√° ng·∫Øn
                continue
                
            text = text.strip()
            
            # T√¨m key g·∫ßn nh·∫•t tr∆∞·ªõc v·ªã tr√≠ n√†y
            best_key = None
            best_key_pos = -1
            for key in keys:
                key_pos = data_str.rfind(key, 0, pos)
                if key_pos > best_key_pos:
                    best_key_pos = key_pos
                    best_key = key
            
            # T·∫°o entry
            entry = {
                'id': entry_id,
                'key': best_key or f'text_entry_{entry_id}',
                'original_text': text,
                'translated_text': text,  # M·∫∑c ƒë·ªãnh gi·ªëng original
                'language': self._detect_language(text),
                'position': pos,
                'length': len(text)
            }
            entries.append(entry)
            processed_texts.add(text)
            entry_id += 1
        
        # L·ªçc b·ªè c√°c entry tr√πng l·∫∑p ho·∫∑c kh√¥ng c√≥ √Ω nghƒ©a
        filtered_entries = []
        seen_texts = set()
        
        for entry in entries:
            text = entry['original_text']
            # B·ªè qua n·∫øu:
            # - Text qu√° ng·∫Øn
            # - Ch·ªâ ch·ª©a k√Ω t·ª± ƒë·∫∑c bi·ªát
            # - ƒê√£ th·∫•y r·ªìi
            if (len(text) < 5 or 
                re.match(r'^[^a-zA-Z]*$', text) or 
                text in seen_texts or
                text.count(' ') == 0 and len(text) < 10):  # T·ª´ ƒë∆°n qu√° ng·∫Øn
                continue
                
            seen_texts.add(text)
            filtered_entries.append(entry)
        
        return filtered_entries
    
    def _detect_language(self, text: str) -> str:
        """Ph√°t hi·ªán ng√¥n ng·ªØ c·ªßa text"""
        # Ki·ªÉm tra c√°c k√Ω t·ª± ƒë·∫∑c trung
        if re.search(r'[√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]', text.lower()):
            return 'vietnamese'
        elif re.search(r'[„Å≤„Çâ„Åå„Å™„Ç´„Çø„Ç´„Éä]', text):
            return 'japanese'
        elif re.search(r'[Í∞Ä-Ìû£]', text):
            return 'korean'
        elif re.search(r'[‰∏Ä-ÈæØ]', text):
            return 'chinese'
        elif re.search(r'[√†√¢√§√©√®√™√´√Ø√Æ√¥√∂√π√ª√º√ø√ß]', text.lower()):
            return 'french'
        elif re.search(r'[√§√∂√º√ü]', text.lower()):
            return 'german'
        elif re.search(r'[√†√®√¨√≤√π√°√©√≠√≥√∫]', text.lower()):
            return 'italian'
        elif re.search(r'[√±√°√©√≠√≥√∫√º]', text.lower()):
            return 'spanish'
        else:
            return 'english'
    
    def export_to_json(self, extracted_data: Dict, output_file: str):
        """Xu·∫•t d·ªØ li·ªáu ra file JSON ƒë·ªÉ ch·ªânh s·ª≠a"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(extracted_data, f, ensure_ascii=False, indent=2)
            print(f"ƒê√£ xu·∫•t d·ªØ li·ªáu ra: {output_file}")
        except Exception as e:
            print(f"L·ªói khi xu·∫•t file JSON: {e}")
    
    def import_from_json(self, json_file: str) -> Dict:
        """ƒê·ªçc d·ªØ li·ªáu ƒë√£ ch·ªânh s·ª≠a t·ª´ file JSON"""
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"L·ªói khi ƒë·ªçc file JSON: {e}")
            return {}
    
    def rebuild_uasset(self, json_data: Dict, output_file: str):
        """T√°i t·∫°o file .uasset v·ªõi text ƒë√£ ch·ªânh s·ª≠a"""
        try:
            # B·∫Øt ƒë·∫ßu v·ªõi d·ªØ li·ªáu g·ªëc
            new_data = self.original_data
            
            # Thay th·∫ø t·ª´ng text entry
            for entry in json_data.get('text_entries', []):
                original_text = entry['original_text']
                translated_text = entry['translated_text']
                
                if original_text != translated_text:
                    # T√¨m v√† thay th·∫ø text trong binary data
                    original_bytes = original_text.encode('utf-8')
                    translated_bytes = translated_text.encode('utf-8')
                    
                    # Thay th·∫ø n·∫øu t√¨m th·∫•y
                    if original_bytes in new_data:
                        new_data = new_data.replace(original_bytes, translated_bytes, 1)
                        print(f"ƒê√£ thay th·∫ø: '{original_text}' -> '{translated_text}'")
            
            # Ghi file m·ªõi
            with open(output_file, 'wb') as f:
                f.write(new_data)
            
            print(f"ƒê√£ t·∫°o file m·ªõi: {output_file}")
            
        except Exception as e:
            print(f"L·ªói khi t√°i t·∫°o file .uasset: {e}")

def batch_extract_all():
    """Tr√≠ch xu·∫•t t·∫•t c·∫£ file .uasset trong folder hi·ªán t·∫°i v√† folder 'original' ra folder 'extract'"""
    extractor = UAssetTextExtractor()
    
    # T·∫°o folder extract n·∫øu ch∆∞a c√≥
    extract_folder = "extract"
    if not os.path.exists(extract_folder):
        os.makedirs(extract_folder)
        print(f"ƒê√£ t·∫°o folder: {extract_folder}")
    
    # T√¨m t·∫•t c·∫£ file .uasset trong folder hi·ªán t·∫°i
    uasset_files = [f for f in os.listdir('.') if f.endswith('.uasset')]
    
    # T√¨m th√™m file .uasset trong folder "original" n·∫øu c√≥
    original_folder = "original"
    if os.path.exists(original_folder):
        original_files = [os.path.join(original_folder, f) for f in os.listdir(original_folder) if f.endswith('.uasset')]
        uasset_files.extend(original_files)
        print(f"T√¨m th·∫•y th√™m {len(original_files)} file trong folder original")
    
    if not uasset_files:
        print("Kh√¥ng t√¨m th·∫•y file .uasset n√†o trong folder hi·ªán t·∫°i v√† folder original")
        return
    
    print(f"T√¨m th·∫•y {len(uasset_files)} file .uasset:")
    for file in uasset_files:
        print(f"  - {file}")
    
    print("\nB·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t...")
    
    success_count = 0
    for uasset_file in uasset_files:
        try:
            print(f"\nüìÅ ƒêang x·ª≠ l√Ω: {uasset_file}")
            
            # Tr√≠ch xu·∫•t text
            extracted_data = extractor.extract_texts(uasset_file)
            
            if extracted_data and extracted_data.get('text_entries'):
                # T·∫°o t√™n file JSON trong folder extract
                json_filename = uasset_file.replace('.uasset', '_texts.json')
                json_path = os.path.join(extract_folder, json_filename)
                
                # Xu·∫•t ra file JSON
                extractor.export_to_json(extracted_data, json_path)
                
                entry_count = len(extracted_data.get('text_entries', []))
                print(f"  ‚úÖ Th√†nh c√¥ng: {entry_count} text entries -> {json_path}")
                success_count += 1
            else:
                print(f"  ‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y text c√≥ √Ω nghƒ©a trong {uasset_file}")
                
        except Exception as e:
            print(f"  ‚ùå L·ªói khi x·ª≠ l√Ω {uasset_file}: {e}")
    
    print(f"\nüéâ Ho√†n th√†nh! ƒê√£ tr√≠ch xu·∫•t th√†nh c√¥ng {success_count}/{len(uasset_files)} file")
    print(f"üìÇ C√°c file JSON ƒë√£ ƒë∆∞·ª£c l∆∞u trong folder: {extract_folder}")

def batch_import_all():
    """Import t·∫•t c·∫£ file JSON t·ª´ folder 'extract' v√† t·∫°o file .uasset m·ªõi trong folder 'import'"""
    extractor = UAssetTextExtractor()
    
    extract_folder = "extract"
    import_folder = "import"
    
    # Ki·ªÉm tra folder extract
    if not os.path.exists(extract_folder):
        print(f"Kh√¥ng t√¨m th·∫•y folder: {extract_folder}")
        print("H√£y ch·∫°y 'batch-extract' tr∆∞·ªõc")
        return
    
    # T·∫°o folder import n·∫øu ch∆∞a c√≥
    if not os.path.exists(import_folder):
        os.makedirs(import_folder)
        print(f"ƒê√£ t·∫°o folder: {import_folder}")
    
    # T√¨m t·∫•t c·∫£ file JSON trong folder extract
    json_files = [f for f in os.listdir(extract_folder) if f.endswith('_texts.json')]
    
    if not json_files:
        print(f"Kh√¥ng t√¨m th·∫•y file JSON n√†o trong folder: {extract_folder}")
        return
    
    print(f"T√¨m th·∫•y {len(json_files)} file JSON:")
    for file in json_files:
        print(f"  - {file}")
    
    print("\nB·∫Øt ƒë·∫ßu import...")
    
    success_count = 0
    for json_file in json_files:
        try:
            json_path = os.path.join(extract_folder, json_file)
            print(f"\nüìÅ ƒêang x·ª≠ l√Ω: {json_file}")
            
            # ƒê·ªçc file JSON
            json_data = extractor.import_from_json(json_path)
            if not json_data:
                print(f"  ‚ùå Kh√¥ng th·ªÉ ƒë·ªçc file JSON: {json_file}")
                continue
            
            # T√¨m file .uasset g·ªëc
            original_uasset = json_data.get('file_info', {}).get('original_file')
            if not original_uasset or not os.path.exists(original_uasset):
                print(f"  ‚ùå Kh√¥ng t√¨m th·∫•y file .uasset g·ªëc cho {json_file}")
                continue
            
            # ƒê·ªçc l·∫°i file g·ªëc
            extractor.extract_texts(original_uasset)
            
            # T·∫°o t√™n file .uasset m·ªõi trong folder import
            original_filename = os.path.basename(original_uasset)
            new_filename = original_filename.replace('.uasset', '_translated.uasset')
            new_path = os.path.join(import_folder, new_filename)
            
            # T·∫°o file .uasset m·ªõi
            extractor.rebuild_uasset(json_data, new_path)
            
            print(f"  ‚úÖ Th√†nh c√¥ng: {new_path}")
            success_count += 1
            
        except Exception as e:
            print(f"  ‚ùå L·ªói khi x·ª≠ l√Ω {json_file}: {e}")
    
    print(f"\nüéâ Ho√†n th√†nh! ƒê√£ import th√†nh c√¥ng {success_count}/{len(json_files)} file")
    print(f"üìÇ C√°c file .uasset m·ªõi ƒë√£ ƒë∆∞·ª£c l∆∞u trong folder: {import_folder}")

def main():
    parser = argparse.ArgumentParser(description='UAsset Text Extractor and Importer')
    parser.add_argument('action', choices=['extract', 'import', 'batch-extract', 'batch-import'], 
                       help='H√†nh ƒë·ªông: extract, import, batch-extract, batch-import')
    parser.add_argument('input_file', nargs='?', help='File ƒë·∫ßu v√†o (.uasset ho·∫∑c .json) - kh√¥ng c·∫ßn cho batch operations')
    parser.add_argument('-o', '--output', help='File ƒë·∫ßu ra')
    
    args = parser.parse_args()
    
    extractor = UAssetTextExtractor()
    
    if args.action == 'batch-extract':
        # Tr√≠ch xu·∫•t t·∫•t c·∫£ file .uasset trong folder hi·ªán t·∫°i
        batch_extract_all()
        
    elif args.action == 'batch-import':
        # Import t·∫•t c·∫£ file t·ª´ folder extract
        batch_import_all()
        
    elif args.action == 'extract':
        # Tr√≠ch xu·∫•t text t·ª´ .uasset
        if not args.input_file:
            print("C·∫ßn ch·ªâ ƒë·ªãnh file ƒë·∫ßu v√†o cho action 'extract'")
            return
            
        if not args.input_file.endswith('.uasset'):
            print("File ƒë·∫ßu v√†o ph·∫£i l√† .uasset")
            return
        
        output_file = args.output or args.input_file.replace('.uasset', '_texts.json')
        
        print(f"ƒêang tr√≠ch xu·∫•t text t·ª´: {args.input_file}")
        extracted_data = extractor.extract_texts(args.input_file)
        
        if extracted_data:
            extractor.export_to_json(extracted_data, output_file)
            print(f"T√¨m th·∫•y {len(extracted_data.get('text_entries', []))} text entries")
        else:
            print("Kh√¥ng th·ªÉ tr√≠ch xu·∫•t d·ªØ li·ªáu")
    
    elif args.action == 'import':
        # Import text ƒë√£ ch·ªânh s·ª≠a v√† t·∫°o .uasset m·ªõi
        if not args.input_file:
            print("C·∫ßn ch·ªâ ƒë·ªãnh file ƒë·∫ßu v√†o cho action 'import'")
            return
            
        if not args.input_file.endswith('.json'):
            print("File ƒë·∫ßu v√†o ph·∫£i l√† .json")
            return
        
        json_data = extractor.import_from_json(args.input_file)
        if not json_data:
            print("Kh√¥ng th·ªÉ ƒë·ªçc file JSON")
            return
        
        original_uasset = json_data.get('file_info', {}).get('original_file')
        if not original_uasset or not os.path.exists(original_uasset):
            print("Kh√¥ng t√¨m th·∫•y file .uasset g·ªëc")
            return
        
        # ƒê·ªçc l·∫°i file g·ªëc
        extractor.extract_texts(original_uasset)
        
        output_file = args.output or original_uasset.replace('.uasset', '_translated.uasset')
        
        print(f"ƒêang t·∫°o file .uasset m·ªõi t·ª´: {args.input_file}")
        extractor.rebuild_uasset(json_data, output_file)

if __name__ == '__main__':
    # V√≠ d·ª• s·ª≠ d·ª•ng n·∫øu ch·∫°y tr·ª±c ti·∫øp
    if len(os.sys.argv) == 1:
        print("V√≠ d·ª• s·ª≠ d·ª•ng:")
        print("1. Tr√≠ch xu·∫•t text: python uasset_text_extractor.py extract GDSSystemText.uasset")
        print("2. Import text: python uasset_text_extractor.py import GDSSystemText_texts.json")
        print("")
        
        # Demo v·ªõi file hi·ªán t·∫°i
        extractor = UAssetTextExtractor()
        demo_file = "/Users/phamminhkha/Desktop/DichGame/GDSSystemText.uasset"
        
        if os.path.exists(demo_file):
            print(f"Demo: Tr√≠ch xu·∫•t text t·ª´ {demo_file}")
            extracted_data = extractor.extract_texts(demo_file)
            
            if extracted_data:
                output_json = demo_file.replace('.uasset', '_texts.json')
                extractor.export_to_json(extracted_data, output_json)
                print(f"ƒê√£ t·∫°o file: {output_json}")
                print(f"T√¨m th·∫•y {len(extracted_data.get('text_entries', []))} text entries")
                
                # Hi·ªÉn th·ªã m·ªôt v√†i v√≠ d·ª•
                print("\nM·ªôt v√†i text entries ƒë·∫ßu ti√™n:")
                for i, entry in enumerate(extracted_data.get('text_entries', [])[:5]):
                    print(f"{i+1}. [{entry['language']}] {entry['original_text'][:50]}...")
    else:
        main()